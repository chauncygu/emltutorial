{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Dataset\n",
    "\n",
    "Let's have a look (e.g.) at the distribution of the number zombies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "data_fname = os.path.join('data', 'za_data.csv')\n",
    "data = pd.read_csv(data_fname)\n",
    "\n",
    "# Some meta-information\n",
    "sim_in = ['edge_ratio', 'inf_prob', 'act_rate', 'rec_rate', 'ds_rate', 'di_rate']\n",
    "sim_out = ['i_num', 'survivors']\n",
    "n_in = len(sim_in)\n",
    "n_out = len(sim_out)\n",
    "in_defaults = [0.004, 0.7, 3, 0, 0.05, 0.00]\n",
    "pop_size = 500\n",
    "\n",
    "# Compute a new column (number of survivors)\n",
    "data['survivors'] = data['s_num'] + data['r_num']\n",
    "\n",
    "# Plot a a few distribution\n",
    "nbins = 20\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.title('Number of zombies')\n",
    "plt.hist(data.i_num, bins=nbins, density=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually heavy tailed! It means that, even if it is very likely that the zombies are few, there is also a non-negligible chance that their number swell beyond control.\n",
    "\n",
    "We can somehow mitigate this by changing the way we aggregate simulator outputs, matching the way we will use that information in the optimization problem. In particular:\n",
    "\n",
    "* Few survivors are a bad thing that we want to avoid, so we can focus on the *minimum* number of survivors for each input configuration\n",
    "* Many zombies are a bad thing that we want to avoid, so we can focus on the *maximum* number of survivors for each input configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate + split into training, validation, and test data\n",
    "data_tr = []\n",
    "data_vl = []\n",
    "data_ts = []\n",
    "for lbl, gdata in data.groupby(sim_in):\n",
    "    lnum = len(gdata)\n",
    "    sep1 = int(lnum * 2/3)\n",
    "    sep2 = int(lnum * 5/6)\n",
    "    gdata_tr = gdata.iloc[:sep1]\n",
    "    gdata_vl = gdata.iloc[sep1:sep2]\n",
    "    gdata_ts = gdata.iloc[sep2:]\n",
    "    data_tr.append(lbl + (gdata_tr['i_num'].max(),) + (gdata_tr['survivors'].min(),))\n",
    "    data_vl.append(lbl + (gdata_vl['i_num'].max(),) + (gdata_vl['survivors'].min(),))\n",
    "    data_ts.append(lbl + (gdata_ts['i_num'].max(),) + (gdata_ts['survivors'].min(),))\n",
    "    \n",
    "data_tr = pd.DataFrame(data=data_tr, columns=sim_in + sim_out)\n",
    "data_vl = pd.DataFrame(data=data_vl, columns=sim_in + sim_out)\n",
    "data_ts = pd.DataFrame(data=data_ts, columns=sim_in + sim_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to standardize and store as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize inputs\n",
    "means_in = data_tr[sim_in].mean(axis=0)\n",
    "stds_in = data_tr[sim_in].std(axis=0)\n",
    "data_tr[sim_in] = (data_tr[sim_in] - means_in) / stds_in\n",
    "data_vl[sim_in] = (data_vl[sim_in] - means_in) / stds_in\n",
    "data_ts[sim_in] = (data_ts[sim_in] - means_in) / stds_in\n",
    "\n",
    "# Standardize output\n",
    "data_tr[sim_out] /= pop_size\n",
    "data_vl[sim_out] /= pop_size\n",
    "data_ts[sim_out] /= pop_size\n",
    "\n",
    "# Store into an HDF5 archive\n",
    "hdf_fname = os.path.join('shared', 'za_processed.h5')\n",
    "with pd.HDFStore(hdf_fname, 'w') as store:\n",
    "    store['data'] = data\n",
    "    store['data_tr'] = data_tr\n",
    "    store['data_vl'] = data_vl\n",
    "    store['data_ts'] = data_ts\n",
    "    store['means_in'] = means_in\n",
    "    store['stds_in'] = stds_in\n",
    "    store['sim_in'] = pd.Series(sim_in)\n",
    "    store['sim_out'] = pd.Series(sim_out)\n",
    "    store['in_defaults'] = pd.Series(index=sim_in, data=in_defaults)\n",
    "    store['meta'] = pd.Series(index=['pop_size'], data=[pop_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Again\n",
    "\n",
    "And then we need to train out networks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Separate input and output\n",
    "X_tr = data_tr[sim_in]\n",
    "Y_tr = data_tr[sim_out]\n",
    "X_vl = data_vl[sim_in]\n",
    "Y_vl = data_vl[sim_out]\n",
    "X_ts = data_ts[sim_in]\n",
    "Y_ts = data_ts[sim_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the scalar outpt networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K\n",
    "import tensorflow\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Seed the RNGs\n",
    "np.random.seed(42)\n",
    "tensorflow.set_random_seed(42)\n",
    "\n",
    "# Custom R2 metric (courtesy of https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/34019)\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Input shape\n",
    "input_shape = (X_tr.shape[1],)\n",
    "\n",
    "# Handle outputs\n",
    "max_epochs = 50\n",
    "for target in sim_out:\n",
    "    y_tr = Y_tr[target].values\n",
    "    y_vl = Y_vl[target].values\n",
    "    y_ts = Y_ts[target].values\n",
    "\n",
    "    # Define a Neural Network model to predict the number of infected\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mse',\n",
    "                  metrics=[r2_score])\n",
    "\n",
    "    # Setup and perform training\n",
    "    weight_fname = os.path.join('shared', 'nn_reg_%s.h5' % target)\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "                 ModelCheckpoint(filepath=weight_fname, monitor='val_loss', save_best_only=True)]\n",
    "    model.fit(X_tr, y_tr, epochs=max_epochs, batch_size=32, callbacks=callbacks,\n",
    "              validation_data=(X_vl, y_vl), verbose=0)\n",
    "    \n",
    "    # Save the model architecture\n",
    "    arch_fname = os.path.join('shared', 'nn_reg_%s.json' % target)\n",
    "    with open(arch_fname, 'w') as fp:\n",
    "        fp.write(model.to_json())\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    print('=== Results for target \"{}\"'.format(target))\n",
    "    res_tr = model.evaluate(X_tr, y_tr, batch_size=len(X_tr))\n",
    "    print('Loss and R2 on the training set: {}, {}'.format(*res_tr))\n",
    "    res_vl = model.evaluate(X_vl, y_vl, batch_size=len(X_vl))\n",
    "    print('Loss and R2 on the validation set: {}, {}'.format(*res_vl) )\n",
    "    res_ts = model.evaluate(X_ts, y_ts, batch_size=len(X_ts))\n",
    "    print('Loss and R2 on the test set: {}, {}'.format(*res_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the one with vector output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import backend as K\n",
    "import tensorflow\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Seed the RNGs\n",
    "np.random.seed(42)\n",
    "tensorflow.set_random_seed(42)\n",
    "\n",
    "# Custom R2 metric (courtesy of https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/34019)\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Input shape\n",
    "input_shape = (X_tr.shape[1],)\n",
    "\n",
    "# Both outputs at the same time\n",
    "max_epochs = 50\n",
    "\n",
    "# Define a Neural Network model to predict the number of infected\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=input_shape))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=[r2_score])\n",
    "\n",
    "# Setup and perform training\n",
    "weight_fname = os.path.join('shared', 'nn_reg.h5')\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath=weight_fname, monitor='val_loss', save_best_only=True)]\n",
    "model.fit(X_tr, Y_tr, epochs=max_epochs, batch_size=32, callbacks=callbacks,\n",
    "          validation_data=(X_vl, Y_vl), verbose=0)\n",
    "\n",
    "# Save the model architecture\n",
    "arch_fname = os.path.join('shared', 'nn_reg.json')\n",
    "with open(arch_fname, 'w') as fp:\n",
    "    fp.write(model.to_json())\n",
    "\n",
    "# Evaluate on the test set\n",
    "res_tr = model.evaluate(X_tr, Y_tr, batch_size=len(X_tr))\n",
    "print('Loss and R2 on the training set: {}, {}'.format(*res_tr))\n",
    "res_vl = model.evaluate(X_vl, Y_vl, batch_size=len(X_vl))\n",
    "print('Loss and R2 on the validation set: {}, {}'.format(*res_vl) )\n",
    "res_ts = model.evaluate(X_ts, Y_ts, batch_size=len(X_ts))\n",
    "print('Loss and R2 on the test set: {}, {}'.format(*res_ts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Round\n",
    "\n",
    "We can finally solve again the problem, with the new networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "\n",
    "# Dynamically add the Empirical Model Learning folder to the python path\n",
    "eml_path = '..'\n",
    "if not eml_path in sys.path:\n",
    "    sys.path.insert(1, eml_path)\n",
    "    \n",
    "# Load the data\n",
    "data_fname = os.path.join('shared', 'za_processed.h5')\n",
    "with pd.HDFStore(data_fname) as store:\n",
    "    data = store['data']\n",
    "    means_in = store['means_in']\n",
    "    stds_in = store['stds_in']\n",
    "    sim_in = store['sim_in']\n",
    "    sim_out = store['sim_out']\n",
    "    in_defaults = store['in_defaults']\n",
    "    pop_size = store['meta']['pop_size']\n",
    "\n",
    "    \n",
    "# Read the NN model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture\n",
    "net_prefixes = ['nn_reg_{}'.format(t) for t in sim_out]\n",
    "knet = {}\n",
    "def load_keras_nets(knet):\n",
    "    # Load scalar output NNs\n",
    "    for target, net_prefix in zip(sim_out, net_prefixes):\n",
    "        net_fname = os.path.join('shared', '{}.json'.format(net_prefix))\n",
    "        with open(net_fname) as fp:\n",
    "            knet[target] = model_from_json(fp.read())\n",
    "\n",
    "        # Load the model weights\n",
    "        wgt_fname = os.path.join('shared', '{}.h5'.format(net_prefix))\n",
    "        knet[target].load_weights(wgt_fname)\n",
    "\n",
    "    # Load vector output NN (this one is available in a single version)\n",
    "    net_fname = os.path.join('shared', 'nn_reg.json')\n",
    "    with open(net_fname) as fp:\n",
    "        knet['all'] = model_from_json(fp.read())\n",
    "\n",
    "    # Load the model weights\n",
    "    wgt_fname = os.path.join('shared', 'nn_reg.h5')\n",
    "    knet['all'].load_weights(wgt_fname)\n",
    "        \n",
    "load_keras_nets(knet)\n",
    "\n",
    "\n",
    "from eml.net import describe as ndescribe\n",
    "from eml.net.reader import keras_reader\n",
    "\n",
    "# Convert the Keras models in the EML format\n",
    "net = {}\n",
    "def convert_keras_net(knet, net):\n",
    "    # Convert scalar-output NNs\n",
    "    for target in sim_out:\n",
    "        net[target] = keras_reader.read_keras_sequential(knet[target])\n",
    "    \n",
    "    # Convert vector-output NN\n",
    "    net['all'] = keras_reader.read_keras_sequential(knet['all'])\n",
    "        \n",
    "convert_keras_net(knet, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage the neuron bounds for the network encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain basic input bounds from out dataset\n",
    "\n",
    "# Compute minima and maxima\n",
    "X_min, X_max = data[sim_in].min(), data[sim_in].max()\n",
    "# Standardize\n",
    "X_min = (X_min - means_in) / stds_in\n",
    "X_max = (X_max - means_in) / stds_in\n",
    "\n",
    "\n",
    "# Compute basic bounds\n",
    "from eml.net import process as nprocess\n",
    "\n",
    "def net_basic_bounds(net):\n",
    "    for target in list(sim_out) + ['all']:\n",
    "        # Reset existing bounds (just to ensure idempotence)\n",
    "        net[target].reset_bounds()\n",
    "\n",
    "        # Enforce basic input bounds\n",
    "        in_layer = net[target].layer(0)\n",
    "        for neuron, lb, ub in zip(in_layer.neurons(), X_min, X_max):\n",
    "            neuron.update_lb(lb)\n",
    "            neuron.update_ub(ub)\n",
    "\n",
    "        # Compute bounds for the hidden neurons via Interval Based Reasoning\n",
    "        nprocess.ibr_bounds(net[target])\n",
    "\n",
    "net_basic_bounds(net)\n",
    "\n",
    "#\n",
    "# Preprocessing: bound tightening\n",
    "#\n",
    "from eml.backend import cplex_backend\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "\n",
    "# Run forward bound tightening\n",
    "timelimit = 10\n",
    "def net_bound_tightening(net, timelimit):\n",
    "    for target in list(sim_out) + ['all']:\n",
    "        nprocess.fwd_bound_tighthening(bkd, net=net[target],\n",
    "                                      timelimit=timelimit)\n",
    "        # Display the bounds\n",
    "        print('=== TARGET: {}'.format(target))\n",
    "        print(net[target])\n",
    "        print()\n",
    "\n",
    "net_bound_tightening(net, timelimit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we model and solve the overall problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load problem data\n",
    "#\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the data about available measures\n",
    "measure_fname = os.path.join('data', 'measures.json')\n",
    "with open(measure_fname) as fp:\n",
    "    mdata = json.load(fp)\n",
    "\n",
    "# Separate the measure effect data from the combinations\n",
    "effects = mdata['effects']\n",
    "combinations = mdata['combinations']\n",
    "\n",
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "cost_var = mdl.continuous_var(name='cost')\n",
    "\n",
    "# Model construction functions\n",
    "def build_inout_vars(bkd, mdl, X_vars, Y_vars):\n",
    "    # Build one variable for each network input\n",
    "    for in_name, lb, ub in zip(sim_in, X_min, X_max):\n",
    "        X_vars.append(mdl.continuous_var(lb=lb, ub=ub, name=in_name))\n",
    "\n",
    "    # Build one variable for each network output\n",
    "    for out_name in sim_out:\n",
    "        # NOTE use slightly larger bounds (to account for approximation errors)\n",
    "        Y_vars.append(mdl.continuous_var(lb=-1, ub=2, name=out_name))\n",
    "\n",
    "\n",
    "def build_nat_out(bkd, mdl, Y_nat):\n",
    "    for i, out_name in enumerate(sim_out):\n",
    "        # Build the natural scale variable\n",
    "        lb = Y_vars[i].lb * pop_size\n",
    "        ub = Y_vars[i].ub * pop_size\n",
    "        ynat = mdl.continuous_var(lb=lb, ub=ub, name=out_name+'_nat')\n",
    "        Y_nat.append(ynat)\n",
    "\n",
    "        # Add the standardization constraints\n",
    "        mdl.add_constraint(Y_nat[i] == Y_vars[i] * pop_size)       \n",
    "\n",
    "\n",
    "def build_nat_in(bkd, mdl, X_nat):\n",
    "    for i, (in_name, lb, ub) in enumerate(zip(sim_in, X_min, X_max)):\n",
    "        # Build the natural scale variable\n",
    "        mean, std = means_in[in_name], stds_in[in_name]\n",
    "        lb_nat = lb * std + mean\n",
    "        ub_nat = ub * std + mean\n",
    "        span = ub_nat - lb_nat\n",
    "        xnat = mdl.continuous_var(lb=lb_nat-span, ub=ub_nat+span,\n",
    "                                  name=in_name+'_nat')\n",
    "        X_nat.append(xnat)\n",
    "\n",
    "        # Add the capping & standardization constraints\n",
    "        nat_nodes = [lb_nat-span, lb_nat, ub_nat, ub_nat+span]\n",
    "        std_nodes = [lb, lb, ub, ub]\n",
    "        util.encode_pwl(bkd, mdl, xvars=[X_nat[i], X_vars[i]],\n",
    "                        nodes=[nat_nodes, std_nodes],\n",
    "                       name=in_name)\n",
    "\n",
    "        \n",
    "def build_measure_vars(bkd, mdl, M_vars, C_vars, M_map):\n",
    "    # Build one binary variable per measure\n",
    "    for i, effect in enumerate(effects):\n",
    "        mvar = mdl.binary_var(name=effect['name'])\n",
    "        M_vars.append(mvar)\n",
    "        M_map[effect['name']] = i\n",
    "\n",
    "    # Build one binary variable per combination\n",
    "    for i, combo in enumerate(combinations):\n",
    "        cvar = mdl.binary_var(name='-'.join(combo['deps']))\n",
    "        C_vars.append(cvar)        \n",
    "\n",
    "\n",
    "def build_dependencies(bkd, mdl):\n",
    "    for i, combo in enumerate(combinations):\n",
    "        ndeps = len(combo['deps'])\n",
    "        mvars = [M_vars[M_map[name]] for name in combo['deps']]\n",
    "        mdl.add_constraint(ndeps * C_vars[i] <= sum(mvars))\n",
    "\n",
    "\n",
    "def build_measure_effect_csts(bkd, mdl):\n",
    "    for i, in_name in enumerate(sim_in):\n",
    "        # Effects to input\n",
    "        coefs, evars = [], []\n",
    "        for j, effect in enumerate(effects):\n",
    "            if in_name in effect:\n",
    "                coefs.append(effect[in_name])\n",
    "                evars.append(M_vars[j])\n",
    "        # Combinations to input\n",
    "        for j, combo in enumerate(combinations):\n",
    "            if in_name in combo:\n",
    "                coefs.append(combo[in_name])\n",
    "                evars.append(C_vars[j])\n",
    "        # Build the connection constraint\n",
    "        mdl.add_constraint(X_nat[i] == in_defaults[i] + mdl.scal_prod(evars, coefs))\n",
    "\n",
    "\n",
    "def build_cost_structure(bkd, mdl, cost_var):\n",
    "    # Measures to cost\n",
    "    coefs, evars = [], []\n",
    "    for i, effect in enumerate(effects):\n",
    "        coefs.append(effect['cost'])\n",
    "        evars.append(M_vars[i])\n",
    "    mdl.add_constraint(cost_var == mdl.scal_prod(evars, coefs))\n",
    "\n",
    "\n",
    "def build_network_encoding(bkd, mdl, mode='scalar'):\n",
    "    if mode == 'scalar':\n",
    "        for i, target in enumerate(sim_out):\n",
    "            nembed.encode(bkd, net[target], mdl, X_vars, Y_vars[i], target)\n",
    "    elif mode == 'vector':\n",
    "        nembed.encode(bkd, net['all'], mdl, X_vars, Y_vars, 'all')\n",
    "\n",
    "        \n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "build_measure_effect_csts(bkd, mdl)\n",
    "# Build and constraint the cost variable\n",
    "build_cost_structure(bkd, mdl, cost_var)\n",
    "# Encode the network\n",
    "build_network_encoding(bkd, mdl, 'vector')\n",
    "\n",
    "# Budget\n",
    "mdl.add_constraint(cost_var <= 70)\n",
    "# Zombies\n",
    "mdl.add_constraint(Y_nat[0] <= 20)\n",
    "# Survivors\n",
    "mdl.set_objective('max', Y_nat[0])\n",
    "\n",
    "# Solve\n",
    "mdl.set_time_limit(30)\n",
    "print('=== Starting the solution process')\n",
    "sol = mdl.solve()\n",
    "\n",
    "if sol is None:\n",
    "    print('No solution found')\n",
    "else:\n",
    "    print('=== SOLUTION DATA')\n",
    "    print('Solution time: {:.2f} (sec)'.format(mdl.solve_details.time))\n",
    "    print('Solver status: {}'.format(sol.solve_details.status))\n",
    "    print('Survivors: {}'.format(sol[Y_nat[1]]))\n",
    "    print('Zombies: {}'.format(sol[Y_nat[0]]))\n",
    "    print('Cost: {}'.format(sol[cost_var]))\n",
    "    print('Chosen measures:')\n",
    "    for x, effect in zip(M_vars, effects):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(effect['name']))\n",
    "    print('Applicable bonuses:')\n",
    "    for x, combo in zip(C_vars, combinations):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(' + '.join(combo['deps'])))\n",
    "    cstring = 'c({})'.format(', '.join('{:.3f}'.format(max(0, sol[x])) for x in X_nat))\n",
    "    print('Evaluation string: {}'.format(cstring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our old solution is no longer viable: we need to increase the budget if we want to keep things under control.\n",
    "\n",
    "We can now evaluate this final solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

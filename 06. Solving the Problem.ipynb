{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Problem\n",
    "\n",
    "Now that we are armed with our ML models (way better than shotguns!) we can finally deal with our nasty little zombie problem.\n",
    "\n",
    "Here's the plan:\n",
    "\n",
    "* We show how to embed a ML model into an optimization model, using the EMLlib\n",
    "* We build (and solve) an optimization model that includes the ML model\n",
    "\n",
    "We will experiment with both the NN and the DT models. As an optimization techniques we will use Mixed Integer Linear Programming, via the well known IBM cplex solver.\n",
    "\n",
    "The [EMLlib](https://github.com/emlopt/emllib) is a python module that implements EML techniques, including encoding, pre- and post-processing functions, and readers for popular ML frameworks. The project has just started and at the momement includes only the bare mininum needed to run this tutorial. More components will however be added in the coming months.\n",
    "\n",
    "Currently, a version of the EMLlib is included in the tutorial code to make it self-contained (but that will change once a proper pip-based installer will be built).\n",
    "\n",
    "First, we need to register the path to the EMLlib. This step will become unnnecessary once the proper pip-based installer will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "\n",
    "# Dynamically add the Empirical Model Learning folder to the python path\n",
    "eml_path = '..'\n",
    "if not eml_path in sys.path:\n",
    "    sys.path.insert(1, eml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we (again) load some of our data (we will need it!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_fname = os.path.join('shared', 'za_processed.h5')\n",
    "with pd.HDFStore(data_fname) as store:\n",
    "    data = store['data']\n",
    "    means_in = store['means_in']\n",
    "    stds_in = store['stds_in']\n",
    "    sim_in = store['sim_in']\n",
    "    sim_out = store['sim_out']\n",
    "    in_defaults = store['in_defaults']\n",
    "    pop_size = store['meta']['pop_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will need in particular the `in_defaults` series, which contains the default value for all our input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Convert Neural Network Models\n",
    "\n",
    "We can now load our NN models, via [the API provided by keras]():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NN model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture\n",
    "net_prefixes = ['nn_reg_{}'.format(t) for t in sim_out]\n",
    "knet = {}\n",
    "def load_keras_nets(knet):\n",
    "    # Load scalar output NNs\n",
    "    for target, net_prefix in zip(sim_out, net_prefixes):\n",
    "        net_fname = os.path.join('shared', '{}.json'.format(net_prefix))\n",
    "        with open(net_fname) as fp:\n",
    "            knet[target] = model_from_json(fp.read())\n",
    "\n",
    "        # Load the model weights\n",
    "        wgt_fname = os.path.join('shared', '{}.h5'.format(net_prefix))\n",
    "        knet[target].load_weights(wgt_fname)\n",
    "\n",
    "    # Load vector output NN (this one is available in a single version)\n",
    "    net_fname = os.path.join('shared', 'nn_reg.json')\n",
    "    with open(net_fname) as fp:\n",
    "        knet['all'] = model_from_json(fp.read())\n",
    "\n",
    "    # Load the model weights\n",
    "    wgt_fname = os.path.join('shared', 'nn_reg.h5')\n",
    "    knet['all'].load_weights(wgt_fname)\n",
    "        \n",
    "load_keras_nets(knet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the EMLlib allows us to convert the keras networks into an internal format with one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eml.net import describe as ndescribe\n",
    "from eml.net.reader import keras_reader\n",
    "\n",
    "# Convert the Keras models in the EML format\n",
    "net = {}\n",
    "def convert_keras_net(knet, net):\n",
    "    # Convert scalar-output NNs\n",
    "    for target in sim_out:\n",
    "        net[target] = keras_reader.read_keras_sequential(knet[target])\n",
    "    \n",
    "    # Convert vector-output NN\n",
    "    net['all'] = keras_reader.read_keras_sequential(knet['all'])\n",
    "        \n",
    "convert_keras_net(knet, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling ML Input Bounds\n",
    "\n",
    "Many of the encodings employed by the EMLlib require **explicit bounds for the input variables of the ML model**.\n",
    "\n",
    "In many practical cases, reasonable bound can be defined based on domain knowledge. Here, for simplicity we will treat the minimum/maximum value of each ML input in the dataset as its true lower/upper bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain basic input bounds from out dataset\n",
    "\n",
    "# Compute minima and maxima\n",
    "X_min, X_max = data[sim_in].min(), data[sim_in].max()\n",
    "# Standardize\n",
    "X_min = (X_min - means_in) / stds_in\n",
    "X_max = (X_max - means_in) / stds_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding a Neural Network in a MILP model requires explicit bounds not only for the network input, but **also for all the hidden neurons**.\n",
    "\n",
    "The EMLlib allows to perform the computation by:\n",
    "\n",
    "1. Explicitly setting bounds on the neurons in the input layer\n",
    "2. Automatically computing bounds for the hidden neurons via Interval Based Reasoning\n",
    "\n",
    "The upper bound for the *activity* of the $i$-th neuron in the $k$-th layer is compued as:\n",
    "$$\n",
    "ub(y_{k,i}) = \\theta_{k,i} + \\sum_{(k-1, j) \\text{ connected to } (k,i)} w_{k,i,j} \\left\\{\\begin{aligned} ub(x_{k-1,j}) & \\text{ if } w_{k,i,j} \\geq 0 \\\\ lb(x_{k-1,j}) & \\text{ otherwise}\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "The upper bound for the neuron output is then computed as:\n",
    "$$\n",
    "ub(x_{k,i}= f(ub(y_{k,i}))\n",
    "$$\n",
    "\n",
    "The lower bound computation is analogous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic bounds\n",
    "from eml.net import process as nprocess\n",
    "\n",
    "def net_basic_bounds(net):\n",
    "    for target in list(sim_out) + ['all']:\n",
    "        # Reset existing bounds (just to ensure idempotence)\n",
    "        net[target].reset_bounds()\n",
    "\n",
    "        # Enforce basic input bounds\n",
    "        in_layer = net[target].layer(0)\n",
    "        for neuron, lb, ub in zip(in_layer.neurons(), X_min, X_max):\n",
    "            neuron.update_lb(lb)\n",
    "            neuron.update_ub(ub)\n",
    "\n",
    "        # Compute bounds for the hidden neurons via Interval Based Reasoning\n",
    "        nprocess.ibr_bounds(net[target])\n",
    "        # Display the bounds\n",
    "        print('=== TARGET: {}'.format(target))\n",
    "        print(net[target])\n",
    "        print()\n",
    "\n",
    "net_basic_bounds(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for the Zombie Apocalypse (Problem Data)\n",
    "\n",
    "It is now time to have a more detailed look **at the rest of the problem definition**, and namely:\n",
    "\n",
    "* Which preparation mesures we can take\n",
    "* How they affect the inputs of the ML model, i.e. the _features_ that describe our system\n",
    "\n",
    "This information is all stored in the `measures.json` file in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load problem data\n",
    "#\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the data about available measures\n",
    "measure_fname = os.path.join('data', 'measures.json')\n",
    "with open(measure_fname) as fp:\n",
    "    mdata = json.load(fp)\n",
    "\n",
    "# Separate the measure effect data from the combinations\n",
    "effects = mdata['effects']\n",
    "combinations = mdata['combinations']\n",
    "\n",
    "print('=== The first two measures and their effects')\n",
    "for effect in effects[:2]:\n",
    "    print(effect)\n",
    "print('')\n",
    "print('=== All measure names')\n",
    "print(', '.join(e['name'] for e in effects))\n",
    "print('')\n",
    "print('=== The first two combination bonuses')\n",
    "for combo in combinations[:2]:\n",
    "    print(combo)\n",
    "print('')\n",
    "print('=== All combinations')\n",
    "print(', '.join('+'.join(c['deps']) for c in combinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each measure has a name, an effect (not necesarily beneficial) on one of more sytem features, and a cost.\n",
    "\n",
    "Some measures can be combined to amplify their effect or counter their disadvantages. For example, equipping our staff with some kind of weapon and givinig them specific training is much more effective than the equipment or the training alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the \"Core Combinatorial Structure\"\n",
    "\n",
    "We can now start to build all the components in the optimization model that are not directly part of our network encoding. We sometimes refer to this collection to the **core combinatorial structure** of the problem, which  highlights alse the emphasis put by EML on *combinatorial* optimization problem.\n",
    "\n",
    "We begin by building the decision variables that correspond to the (standardized/normalized) input/output of our Neural Networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "\n",
    "# Build input and output variables\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "def build_inout_vars(bkd, mdl, X_vars, Y_vars):\n",
    "    # Build one variable for each network input\n",
    "    for in_name, lb, ub in zip(sim_in, X_min, X_max):\n",
    "        X_vars.append(mdl.continuous_var(lb=lb, ub=ub, name=in_name))\n",
    "\n",
    "    # Build one variable for each network output\n",
    "    for out_name in sim_out:\n",
    "        # NOTE use slightly larger bounds (to account for approximation errors)\n",
    "        Y_vars.append(mdl.continuous_var(lb=-1, ub=2, name=out_name))\n",
    "\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "\n",
    "print(cplex_backend.model_to_string(mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is designed using the [docplex]() modeling interface.\n",
    "\n",
    "The same technology is also employed by the cplex backend of the EMLlib, which we will need to encode the NNs.\n",
    "\n",
    "For sake of clarity it is usuful to introduce a \"natural scale\" (i.e non standardized/normalize) version of the NN input and output.\n",
    "\n",
    "In principle, these will need to be connected to the actual NN input/output varibles via simple linear constraints that correspond to the standardization/normalization. This is actually true only for the *output* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "\n",
    "# Build a natural scale version of each output variable\n",
    "from eml import util\n",
    "importlib.reload(util)\n",
    "Y_nat = []\n",
    "def build_nat_out(bkd, mdl, Y_nat):\n",
    "    for i, out_name in enumerate(sim_out):\n",
    "        # Build the natural scale variable\n",
    "        lb = Y_vars[i].lb * pop_size\n",
    "        ub = Y_vars[i].ub * pop_size\n",
    "        ynat = mdl.continuous_var(lb=lb, ub=ub, name=out_name+'_nat')\n",
    "        Y_nat.append(ynat)\n",
    "\n",
    "        # Add the standardization constraints\n",
    "        mdl.add_constraint(Y_nat[i] == Y_vars[i] * pop_size)\n",
    "\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "\n",
    "print(cplex_backend.model_to_string(mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input variables are slighly trickier to handle, because it may happen that stacking effects of the measures we chose end up pushing one standardized/normalized variable out of its bounds.\n",
    "\n",
    "Whenever this happens, a MILP solver would consider such solution infeasible. In practice, it is more reasonable to think that in such a case the variable reaches a plateau.\n",
    "\n",
    "In other words, the natural scale input variables are connected to their standarized/normalized counterparts both by standardization/normalization constraints *and by capping constraints*.\n",
    "\n",
    "These can be modeled in a compact form via a piecewise linear function with this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot([-2, -1, 1, 2], [-1, -1, 1, 1])\n",
    "plt.xlabel('Natural scale variable')\n",
    "plt.ylabel('Standardized/normalized variable')\n",
    "plt.xticks([-2, -1, 1, 2], ['relaxed min', 'std/norm min', 'std/norm max', 'relaxed max'])\n",
    "plt.yticks([-1, 1], ['std/norm min', 'std/norm max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMLlib provides an easy way to encode piecewise linear functions. This is actually just a coincidence, becasue this type of function plays a role in some encodings.\n",
    "\n",
    "The method is the `encode_pwl` function from the `util` module. The function requires to specify:\n",
    "\n",
    "* The backend object to be used for the encoding\n",
    "* The model where the function should be inserted\n",
    "* The variables that should be connected via a piecewise linear function\n",
    "* A list of nodes, i.e. the vector of x and y coordinates in the plot above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "\n",
    "# Build a natural scale version of each input variable\n",
    "X_nat = []\n",
    "def build_nat_in(bkd, mdl, X_nat):\n",
    "    for i, (in_name, lb, ub) in enumerate(zip(sim_in, X_min, X_max)):\n",
    "        # Build the natural scale variable\n",
    "        mean, std = means_in[in_name], stds_in[in_name]\n",
    "        lb_nat = lb * std + mean\n",
    "        ub_nat = ub * std + mean\n",
    "        span = ub_nat - lb_nat\n",
    "        xnat = mdl.continuous_var(lb=lb_nat-span, ub=ub_nat+span,\n",
    "                                  name=in_name+'_nat')\n",
    "        X_nat.append(xnat)\n",
    "\n",
    "        # Add the capping & standardization constraints\n",
    "        nat_nodes = [lb_nat-span, lb_nat, ub_nat, ub_nat+span]\n",
    "        std_nodes = [lb, lb, ub, ub]\n",
    "        util.encode_pwl(bkd, mdl, xvars=[X_nat[i], X_vars[i]],\n",
    "                        nodes=[nat_nodes, std_nodes],\n",
    "                       name=in_name)\n",
    "\n",
    "build_nat_in(bkd, mdl, X_nat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build a binary variable for each measure, and one for each combination of measures that is associated to an extra effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "\n",
    "# Build variables for the measures and their combinations\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "def build_measure_vars(bkd, mdl, M_vars, C_vars, M_map):\n",
    "    # Build one binary variable per measure\n",
    "    for i, effect in enumerate(effects):\n",
    "        mvar = mdl.binary_var(name=effect['name'])\n",
    "        M_vars.append(mvar)\n",
    "        M_map[effect['name']] = i\n",
    "\n",
    "    # Build one binary variable per combination\n",
    "    for i, combo in enumerate(combinations):\n",
    "        cvar = mdl.binary_var(name='-'.join(combo['deps']))\n",
    "        C_vars.append(cvar)\n",
    "\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination variables can be set to 1 only if all the basic measures they depend have been chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "\n",
    "# Build dependency constraints\n",
    "def build_dependencies(bkd, mdl):\n",
    "    for i, combo in enumerate(combinations):\n",
    "        ndeps = len(combo['deps'])\n",
    "        mvars = [M_vars[M_map[name]] for name in combo['deps']]\n",
    "        mdl.add_constraint(ndeps * C_vars[i] <= sum(mvars))\n",
    "\n",
    "build_dependencies(bkd, mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify that each NN input (i.e. feature that descibe the system) is given by its defalt value, modified by all the measures that can affect it and have been selected, and by all applicable combinations. In oter words:\n",
    "$$\n",
    "feat_i = default(feat_i) + \\sum_{j \\in \\text{ measures}} eff_{i,j} x_j + \\sum_{j \\in \\text{ combinations}} eff_{i,j} y_j\n",
    "$$\n",
    "\n",
    "Where $x$ refers here to the measure variables and $y$ to the combination variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "def build_measure_effect_csts(bkd, mdl):\n",
    "    for i, in_name in enumerate(sim_in):\n",
    "        # Effects to input\n",
    "        coefs, evars = [], []\n",
    "        for j, effect in enumerate(effects):\n",
    "            if in_name in effect:\n",
    "                coefs.append(effect[in_name])\n",
    "                evars.append(M_vars[j])\n",
    "        # Combinations to input\n",
    "        for j, combo in enumerate(combinations):\n",
    "            if in_name in combo:\n",
    "                coefs.append(combo[in_name])\n",
    "                evars.append(C_vars[j])\n",
    "        # Build the connection constraint\n",
    "        mdl.add_constraint(X_nat[i] == in_defaults[i] + mdl.scal_prod(evars, coefs))\n",
    "\n",
    "build_measure_effect_csts(bkd, mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ca adopt a similar approach to compute the overall cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "build_measure_effect_csts(bkd, mdl)\n",
    "\n",
    "# Build and constraint the cost variable\n",
    "cost_var = mdl.continuous_var(name='cost')\n",
    "def build_cost_structure(bkd, mdl, cost_var):\n",
    "    # Measures to cost\n",
    "    coefs, evars = [], []\n",
    "    for i, effect in enumerate(effects):\n",
    "        coefs.append(effect['cost'])\n",
    "        evars.append(M_vars[i])\n",
    "    mdl.add_constraint(cost_var == mdl.scal_prod(evars, coefs))\n",
    "\n",
    "build_cost_structure(bkd, mdl, cost_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding our NN Models\n",
    "\n",
    "We can embed our well-earned MM models! Using the EMLlib, this is in fact quite easy. In detail:\n",
    "\n",
    "* The `encode` function in the `net.embed` modules does all the work\n",
    "* We just need to pass a backend, the network to be encoded, the target model object, and the input/output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "cost_var = mdl.continuous_var(name='cost')\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "build_measure_effect_csts(bkd, mdl)\n",
    "# Build and constraint the cost variable\n",
    "build_cost_structure(bkd, mdl, cost_var)\n",
    "\n",
    "# Encode the network\n",
    "def build_network_encoding(bkd, mdl, mode='scalar'):\n",
    "    if mode == 'scalar':\n",
    "        for i, target in enumerate(sim_out):\n",
    "            nembed.encode(bkd, net[target], mdl, X_vars, Y_vars[i], target)\n",
    "    elif mode == 'vector':\n",
    "        nembed.encode(bkd, net['all'], mdl, X_vars, Y_vars, 'all')\n",
    "        \n",
    "build_network_encoding(bkd, mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Solution\n",
    "\n",
    "Now we just need to specify our \"goal\" constraints and the objective function. In particular we want:\n",
    "\n",
    "* A total cost below a given threshold\n",
    "* A very small final number of zombies\n",
    "* The largest possible number of survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "cost_var = mdl.continuous_var(name='cost')\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "build_measure_effect_csts(bkd, mdl)\n",
    "# Build and constraint the cost variable\n",
    "build_cost_structure(bkd, mdl, cost_var)\n",
    "# Encode the network\n",
    "build_network_encoding(bkd, mdl, 'vector')\n",
    "\n",
    "# Budget\n",
    "mdl.add_constraint(cost_var <= 50)\n",
    "# Zombies\n",
    "mdl.add_constraint(Y_nat[0] <= 20)\n",
    "# Survivors\n",
    "mdl.set_objective('max', Y_nat[0])\n",
    "\n",
    "# Solve\n",
    "mdl.set_time_limit(30)\n",
    "print('=== Starting the solution process')\n",
    "sol = mdl.solve()\n",
    "\n",
    "if sol is None:\n",
    "    print('No solution found')\n",
    "else:\n",
    "    print('=== SOLUTION DATA')\n",
    "    print('Solution time: {:.2f} (sec)'.format(mdl.solve_details.time))\n",
    "    print('Solver status: {}'.format(sol.solve_details.status))\n",
    "    print('Survivors: {}'.format(sol[Y_nat[1]]))\n",
    "    print('Zombies: {}'.format(sol[Y_nat[0]]))\n",
    "    print('Cost: {}'.format(sol[cost_var]))\n",
    "    print('Chosen measures:')\n",
    "    for x, effect in zip(M_vars, effects):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(effect['name']))\n",
    "    print('Applicable bonuses:')\n",
    "    for x, combo in zip(C_vars, combinations):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(' + '.join(combo['deps'])))\n",
    "    cstring = 'c({})'.format(', '.join('{:.3f}'.format(max(0, sol[x])) for x in X_nat))\n",
    "    print('Evaluation string: {}'.format(cstring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency Issues\n",
    "\n",
    "It turns out that solving the problem is as easy as it originally looked!\n",
    "\n",
    "This is largely due to the complexity of handling the NN encoding, even if the network is not by itself large.\n",
    "\n",
    "It turns out that NNs have a particularly nasty combination of characteristics from the perspective of a MILP encoding:\n",
    "\n",
    "* Each fully connected layer leads to a dense block in the coefficient matrix, something that most MILP solver really *don't* like\n",
    "* Linearizing the ReLU activation functions requires using a big-M, whose value depends on the bound computed in our pre-processing step. The weaker the bounds, the weaker the encoding. Since the bound quality decreases with the network depth, deeper networks are hard to deal with.\n",
    "\n",
    "Is there something we can do? Yes, definitely. We can:\n",
    "\n",
    "1. Use a simpler network!\n",
    "   * If we do, we may loose some accuracy, but hopefully not too much: our solution will be approximated in any case.\n",
    "   * In general, **there is always a trade-off between the complexity and accuracy of the ML model and how efficiently it can be managed by the solver**\n",
    "2. We can **compute better bounds**!\n",
    "   * A nice approach consists in repeatedly solving a MILP model that consists in just the network encoding\n",
    "   * Every time, we change the objective function so that it matches either the lower or the upper bound of a neuron\n",
    "   * This process can be performed in a forwad fashion, by exploiting the feed-forward nature of the network\n",
    "   \n",
    "The approach is not particularly complex, and it is well described in [this paper by Matteo Fischetti et al.](https://arxiv.org/abs/1712.06174).\n",
    "\n",
    "In the EMLlib, it is implemented in the `fwd_bound_tightening` function from the `net.process` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Preprocessing: bound tightening\n",
    "#\n",
    "from eml.backend import cplex_backend\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "\n",
    "# Run forward bound tightening\n",
    "timelimit = 10\n",
    "def net_bound_tightening(net, timelimit):\n",
    "    for target in list(sim_out) + ['all']:\n",
    "        nprocess.fwd_bound_tighthening(bkd, net=net[target],\n",
    "                                      timelimit=timelimit)\n",
    "        # Display the bounds\n",
    "        print('=== TARGET: {}'.format(target))\n",
    "        print(net[target])\n",
    "        print()\n",
    "\n",
    "net_bound_tightening(net, timelimit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new bounds are much tighter than the old ones!\n",
    "\n",
    "It takes some time to generate them, but **we need to perform this step only once per model**. Then, we can use the model with tighter bound to define as many optimization problems we wish.\n",
    "\n",
    "We can now try to solve the problem again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and solve a CPLEX model\n",
    "#\n",
    "\n",
    "from eml.backend import cplex_backend\n",
    "import docplex.mp.model as cpx\n",
    "from eml.net import embed as nembed\n",
    "from eml import util\n",
    "\n",
    "# Build a backend object\n",
    "bkd = cplex_backend.CplexBackend()\n",
    "# Build a docplex model\n",
    "mdl = cpx.Model()\n",
    "X_vars = []\n",
    "Y_vars = []\n",
    "Y_nat = []\n",
    "X_nat = []\n",
    "M_vars = []\n",
    "C_vars = []\n",
    "M_map = {}\n",
    "cost_var = mdl.continuous_var(name='cost')\n",
    "\n",
    "# Build input and output variables\n",
    "build_inout_vars(bkd, mdl, X_vars, Y_vars)\n",
    "# Build a natural scale version of each output variable\n",
    "build_nat_out(bkd, mdl, Y_nat)\n",
    "# Build a natural scale version of each input variable\n",
    "build_nat_in(bkd, mdl, X_nat)\n",
    "# Build variables for the measures and their combinations\n",
    "build_measure_vars(bkd, mdl, M_vars, C_vars, M_map)\n",
    "# Build dependency constraints\n",
    "build_dependencies(bkd, mdl)\n",
    "# Connect effects and combinations to the natural-scale network inputs\n",
    "build_measure_effect_csts(bkd, mdl)\n",
    "# Build and constraint the cost variable\n",
    "build_cost_structure(bkd, mdl, cost_var)\n",
    "# Encode the network\n",
    "build_network_encoding(bkd, mdl, 'vector')\n",
    "\n",
    "# Budget\n",
    "mdl.add_constraint(cost_var <= 50)\n",
    "# Zombies\n",
    "mdl.add_constraint(Y_nat[0] <= 20)\n",
    "# Survivors\n",
    "mdl.set_objective('max', Y_nat[0])\n",
    "\n",
    "# Solve\n",
    "mdl.set_time_limit(30)\n",
    "print('=== Starting the solution process')\n",
    "sol = mdl.solve()\n",
    "\n",
    "if sol is None:\n",
    "    print('No solution found')\n",
    "else:\n",
    "    print('=== SOLUTION DATA')\n",
    "    print('Solution time: {:.2f} (sec)'.format(mdl.solve_details.time))\n",
    "    print('Solver status: {}'.format(sol.solve_details.status))\n",
    "    print('Survivors: {}'.format(sol[Y_nat[1]]))\n",
    "    print('Zombies: {}'.format(sol[Y_nat[0]]))\n",
    "    print('Cost: {}'.format(sol[cost_var]))\n",
    "    print('Chosen measures:')\n",
    "    for x, effect in zip(M_vars, effects):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(effect['name']))\n",
    "    print('Applicable bonuses:')\n",
    "    for x, combo in zip(C_vars, combinations):\n",
    "        if sol[x] > 0:\n",
    "            print('* {}'.format(' + '.join(combo['deps'])))\n",
    "    cstring = 'c({})'.format(', '.join('{:.3f}'.format(max(0, sol[x])) for x in X_nat))\n",
    "    print('Evaluation string: {}'.format(cstring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to test the solution we have just obtained: copy the \"evaluation string\" in the the output above nad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
